# @package _global_
defaults:
  - /module: emgformer
  - override /module/head: vq_discrete
  - override /module/decoder: transformer/preset/small
  - override /lr_scheduler: warmup_cosine
  - _self_

task_type: discrete
ignore_index: -100
loss_weights:
  cls: 1.0
num_workers: 8
# Algorithm settings
module:
  provide_initial_pos: False
  decoder:
    causal: true
    preset: small
  head:
    in_channels: 128
    embed_dim: 16
    num_codes: 256
    num_levels: 20
    checkpoint: /home/xiziheng/develop/emg2pose/logs/vqvae/2025-12-20/17-41-20/lightning_logs/version_0/checkpoints/last.ckpt
optimizer:
  lr: .00001
datamodule:
  window_length: 10_000
  val_test_window_length: 10_000
  norm_mode: batch
  # stride: 5_000
  max_open_files: 8

trainer:
  gradient_clip_val: 1
  devices: [0,1,2]
  precision: bf16-mixed
  max_epochs: 300
  strategy: 'ddp_find_unused_parameters_true'

batch_size: 400
matmul_precision: high

transforms:
  train:
    - _target_: emg2pose.transforms.ExtractField
      field: "emg"
    - _target_: emg2pose.transforms.RotationAugmentation
      channel_dim: -1
    - _target_: emg2pose.transforms.ToFloatTensor
    - _target_: emg2pose.transforms.RandomChannelMask
      mask_prob: 0.5
      min_masked: 0
      max_masked: null
      channel_dim: -1
      mask_value: 0.0
    - _target_: emg2pose.transforms.RandomTimeMask
      max_mask_size: 1000
      num_masks: 2
      min_mask_size: 0
      time_dim: 0
      mask_value: 0.0
      mask_prob: 1.0
    - _target_: emg2pose.transforms.RandomFrequencyMask
      max_mask_size: 128
      num_masks: 2
      min_mask_size: 16
      time_dim: 0
      mask_prob: 1.0
  val:
    - ${to_tensor}
  test: ${transforms.val}
