# @package _global_
defaults:
  - /module: emgformer
  - override /module/decoder: transformer/preset/large
  - override /lr_scheduler: warmup_cosine
  - _self_

# Algorithm settings
module:
  provide_initial_pos: False
  decoder:
    causal: true
    preset: large
  head:
    in_channels: 384
# Hyperparameters
optimizer:
  lr: .0003
datamodule:
  # Adding 1790 samples, which is the left_context of the tds network
  window_length: 11_790
  val_test_window_length: 11_790
  norm_mode: batch
  stride: 5_000
checkpoint: /home/xiziheng/develop/emg2pose/logs/2025-12-18/10-27-04/lightning_logs/version_0/checkpoints/best.ckpt
trainer:
  gradient_clip_val: 1
  devices: [0,1,2]
  precision: bf16-mixed
  max_epochs: 200

batch_size: 400
matmul_precision: high

to_tensor:
  _target_: emg2pose.transforms.ExtractToTensor
  field: emg

transforms:
  train:
    - _target_: emg2pose.transforms.ExtractField
      field: "emg"
    - _target_: emg2pose.transforms.RotationAugmentation
      channel_dim: -1
    - _target_: emg2pose.transforms.ToFloatTensor
    - _target_: emg2pose.transforms.RandomChannelMask
      mask_prob: 0.5
      min_masked: 0
      max_masked: null
      channel_dim: -1
      mask_value: 0.0
    - _target_: emg2pose.transforms.RandomTimeMask
      max_mask_size: 1000
      num_masks: 2
      min_mask_size: 0
      time_dim: 0
      mask_value: 0.0
      mask_prob: 1.0
    - _target_: emg2pose.transforms.RandomFrequencyMask
      max_mask_size: 128
      num_masks: 2
      min_mask_size: 16
      time_dim: 0
      mask_prob: 1.0
  val:
    - ${to_tensor}
  test: ${transforms.val}
