# @package _global_
defaults:
  - /module: vemg_with_initial_state
  - override /lr_scheduler: warmup_cosine
  - _self_

# Algorithm settings
provide_initial_pos: False
module:
  state_condition: True
  head:
    _target_: emg2pose.models.heads.mlp.MLPHead
    in_channels: 20
    out_channels: 20

# Hyperparameters
optimizer:
  lr: .001
datamodule:
  # Adding 1790 samples, which is the left_context of the tds network
  window_length: 11_790
  val_test_window_length: 11_790
  max_open_files: 4
  norm_mode: batch
  norm_stats_path: /home/xiziheng/develop/emg2pose/assets/emg_norm_stats.npz
trainer:
  gradient_clip_val: 1
  devices: [0]
  precision: 32
  max_epochs: 100
batch_size: 480
matmul_precision: high

transforms:
  train:
    - _target_: emg2pose.transforms.ExtractField
      field: "emg"
    - _target_: emg2pose.transforms.RotationAugmentation
      channel_dim: -1
    - _target_: emg2pose.transforms.ToFloatTensor
    - _target_: emg2pose.transforms.RandomChannelMask
      mask_prob: 0.5
      min_masked: 0
      max_masked: null
      channel_dim: -1
      mask_value: 0.0
    - _target_: emg2pose.transforms.RandomTimeMask
      max_mask_size: 1000
      num_masks: 2
      min_mask_size: 0
      time_dim: 0
      mask_value: 0.0
      mask_prob: 1.0
    - _target_: emg2pose.transforms.RandomFrequencyMask
      max_mask_size: 128
      num_masks: 2
      min_mask_size: 16
      time_dim: 0
      mask_prob: 1.0
  val:
    - ${to_tensor}
  test: ${transforms.val}