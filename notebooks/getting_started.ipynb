{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea5b86f-8e52-4d44-86d0-fd9e9596f79f",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook showcases basic functionality of the code base.\n",
    "\n",
    "Here, we load the metadata, an example dataset, and run inference using a pre-trained model. \n",
    "\n",
    "We also show how to visualize the joint angle predictions using a hand mesh (requires the UmeTrack package -- see README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9151a71-0405-4c56-8f08-5b521d65b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92da9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DOWNLOAD_DIR = \"/home/xiziheng/develop/emg2pose/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68021559",
   "metadata": {},
   "source": [
    "## Download Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {DATA_DOWNLOAD_DIR} && curl https://fb-ctrl-oss.s3.amazonaws.com/emg2pose/emg2pose_metadata.csv -o emg2pose_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78b0ad1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m metadata_df = pd.read_csv(\u001b[43mDATA_DOWNLOAD_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memg2pose_metadata.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m)\n\u001b[32m      4\u001b[39m metadata_df.head(\u001b[32m5\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_df = pd.read_csv(DATA_DOWNLOAD_DIR / \"emg2pose_metadata.csv\")\n",
    "metadata_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f4199",
   "metadata": {},
   "source": [
    "## Download a Smaller (~600 MiB) Version of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {DATA_DOWNLOAD_DIR} && curl \"https://fb-ctrl-oss.s3.amazonaws.com/emg2pose/emg2pose_dataset_mini.tar\" -o emg2pose_dataset_mini.tar\n",
    "\n",
    "# Unpack the tar to ~/emg2pose_dataset_mini\n",
    "!tar -xvf emg2pose_dataset_mini.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a720e17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-10_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-10_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-11_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-11_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-12_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-12_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-13_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-13_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-14_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-14_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-15_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-15_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-1_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-1_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-2_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-2_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-3_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-3_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-4_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-4_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-5_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-5_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-6_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-6_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-7_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-7_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-8_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-8_right.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-9_left.hdf5',\n",
       " '/home/xiziheng/develop/emg2pose/data/emg2pose_dataset_mini/2022-12-06-1670313600-e3096-cv-emg-pose-train@2-recording-9_right.hdf5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "sessions = sorted(glob.glob(os.path.join(DATA_DOWNLOAD_DIR, \"emg2pose_dataset_mini/*.hdf5\")))\n",
    "sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2f73d",
   "metadata": {},
   "source": [
    "## Let's Look at a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef869193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emg2pose.data import Emg2PoseSessionData\n",
    "\n",
    "session = sessions[15]\n",
    "data = Emg2PoseSessionData(hdf5_path=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee528ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.fields)\n",
    "print()\n",
    "\n",
    "print(f\"{'emg shape: ':<20} {data['emg'].shape}\")\n",
    "print(f\"{'joint_angles shape: ':<20} {data['joint_angles'].shape}\")\n",
    "print(f\"{'time shape: ':<20} {data['time'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e63b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[metadata_df[\"filename\"] == data.metadata[\"filename\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emg2pose.visualization as visualization\n",
    "\n",
    "visualization.ik_failure_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da74733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emg2pose.utils import downsample\n",
    "import numpy as np\n",
    "\n",
    "joint_angles = data[\"joint_angles\"]\n",
    "joint_angles_30hz = downsample(joint_angles, native_fs=2000, target_fs=30)\n",
    "\n",
    "assert not np.any(np.isnan(joint_angles_30hz))\n",
    "\n",
    "visualization.plot_hand_mesh(joint_angles_30hz[100], auto_range=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88485623-7c6a-4487-b4da-c7d6c7263761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ab10b-76a2-4f47-aa02-1df734c62123",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.get_plotly_animation_for_joint_angles(joint_angles_30hz[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994b17c",
   "metadata": {},
   "source": [
    "### Render the Plotly Animation to Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f61f9-ee3d-4a9a-885e-1dfe8bca6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy\n",
    "\n",
    "frames = visualization.joint_angles_to_frames_parallel(joint_angles_30hz[0:250])\n",
    "frames = visualization.remove_alpha_channel(frames)\n",
    "mediapy.show_video(frames, width=800, fps=30, downsample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df732445",
   "metadata": {},
   "source": [
    "## Let's Load a Checkpoint and Generate some Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {DATA_DOWNLOAD_DIR} \\\n",
    "&& curl \"https://fb-ctrl-oss.s3.amazonaws.com/emg2pose/emg2pose_model_checkpoints.tar.gz\" -o emg2pose_model_checkpoints.tar.gz && \\\n",
    "tar -xvzf emg2pose_model_checkpoints.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364ac6ac-d025-4f87-9903-0ff963b9a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emg2pose.utils import generate_hydra_config_from_overrides\n",
    "config = generate_hydra_config_from_overrides(\n",
    "    overrides=[\n",
    "        \"experiment=tracking_vemg2pose\",\n",
    "        f\"checkpoint=/home/xiziheng/develop/emg2pose/emg2pose_model_checkpoints/regression_vemg2pose.ckpt\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c605e9-7bdf-4f58-93c4-25d07d06d9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.6 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../emg2pose_model_checkpoints/regression_vemg2pose.ckpt`\n"
     ]
    }
   ],
   "source": [
    "from emg2pose.lightning import Emg2PoseModule\n",
    "\n",
    "module = Emg2PoseModule.load_from_checkpoint(\n",
    "    config.checkpoint,\n",
    "    network=config.network,\n",
    "    optimizer=config.optimizer,\n",
    "    lr_scheduler=config.lr_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2f80ce-2fe3-4751-b76b-0278cb4b440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = data\n",
    "start_idx = 0\n",
    "stop_idx = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f778327-50d2-4358-8ba6-00ff3d996b0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWarning! Ground truth not available at first time step!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# BCT --> TC (as numpy)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m preds = \u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m joint_angles = joint_angles[\u001b[32m0\u001b[39m].T.detach().numpy()\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "session_window = session[start_idx:stop_idx]\n",
    "\n",
    "# no_ik_failure is not a field so we slice separately\n",
    "no_ik_failure_window = session.no_ik_failure[start_idx:stop_idx]\n",
    "\n",
    "batch = {\n",
    "    \"emg\": torch.Tensor([session_window[\"emg\"].T]).cuda(),  # BCT\n",
    "    \"joint_angles\": torch.Tensor([session_window[\"joint_angles\"].T]).cuda(),  # BCT\n",
    "    \"no_ik_failure\": torch.Tensor([no_ik_failure_window]).cuda(),  # BT\n",
    "}\n",
    "\n",
    "preds, joint_angles, no_ik_failure = module.forward(batch)\n",
    "\n",
    "# Algorithms that use the initial state for ground truth will do poorly\n",
    "# when the first joint angles are missing!\n",
    "if (joint_angles[:, 0] == 0).all():\n",
    "    print(\n",
    "        \"Warning! Ground truth not available at first time step!\"\n",
    "    )\n",
    "\n",
    "# BCT --> TC (as numpy)\n",
    "preds = preds[0].cpu().T.detach().numpy()\n",
    "joint_angles = joint_angles[0].cpu().T.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf1cdd-585f-439f-a17d-37c2ed055bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e29cc-1a96-4c57-8f35-800b742c2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_angles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96c08b-1151-4d8d-a992-f8659530f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_angles_30hz = downsample(joint_angles, native_fs = 2000, target_fs = 30)\n",
    "visualization.get_plotly_animation_for_joint_angles(joint_angles_30hz[0:250], color=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d79b6-a1af-4be4-aebe-979ba1187af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_30hz = downsample(preds, native_fs=2000, target_fs=30)\n",
    "visualization.get_plotly_animation_for_joint_angles(preds_30hz[0:250], color=\"lightpink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55167a87",
   "metadata": {},
   "source": [
    "### Compare the Ground Truth and Predictions Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955efe7-17d7-4d06-a533-b0a504787cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_frames = visualization.joint_angles_to_frames_parallel(joint_angles_30hz[0:250], color=\"gray\")\n",
    "pred_frames = visualization.joint_angles_to_frames_parallel(preds_30hz[0:250], color=\"lightpink\")\n",
    "\n",
    "gt_frames = visualization.remove_alpha_channel(gt_frames)\n",
    "pred_frames = visualization.remove_alpha_channel(pred_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc05a43-3146-4405-a074-ea0d55f28271",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediapy.show_videos(dict(gt=gt_frames, pred=pred_frames), width=400, fps=30, downsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f56de8-3207-46d8-b55f-6328254c7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COLS = 2\n",
    "N_ROWS = 10\n",
    "\n",
    "fig, axs = plt.subplots(N_ROWS, N_COLS, figsize=(4*N_COLS, 2*N_ROWS))\n",
    "\n",
    "axs_flattened = axs.flatten()\n",
    "for i, ax in enumerate(axs_flattened):\n",
    "    ax.set_title(f\"Joint Angle {i}\")\n",
    "    ax.plot(joint_angles_30hz[:, i], label=\"gt\")\n",
    "    ax.plot(preds_30hz[:, i], label=\"pred\")\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle(\"Predicted vs. Ground Truth Joint Angles\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg2pose_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
